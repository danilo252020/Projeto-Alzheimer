# -*- coding: utf-8 -*-
"""Projeto Alzheimer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rjE_cN0rL2kNZkJQK2L1I4HinOLeg6dd
"""

import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageEnhance, ImageFilter
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score

from google.colab import drive

# Montar o Google Drive
drive.mount('/content/drive')

# Defina o diretório base no Google Drive onde as imagens estão organizadas por classe
base_dir = '/content/drive/MyDrive/Alzheimer'  # ajuste o caminho conforme necessário
classes = ['Demente Leve', 'Moderado Demente', 'Não Demente', 'Muito Leve Demente']

# Defina a função de pré-processamento
def preprocess_image(image_path):
    image = Image.open(image_path).convert('RGB')
    image = image.resize((128, 128))  # Redimensiona a imagem para 128x128
    #image = ImageOps.equalize(image)  # Equalização de histograma
    #image = image.filter(ImageFilter.MedianFilter(size=3))  # Remover ruído usando um filtro de suavização
    image_array = np.array(image)
    image_array = image_array / 255.0  # Normaliza a imagem
    return image_array

# Carregar e pré-processar todas as imagens
images = []
labels = []

for label, class_name in enumerate(classes):
    class_dir = os.path.join(base_dir, class_name)
    for filename in os.listdir(class_dir):
        if filename.endswith('.jpg') or filename.endswith('.png'):
            image_path = os.path.join(class_dir, filename)
            image = preprocess_image(image_path)
            images.append(image)
            labels.append(label)

# Converter listas para arrays numpy
images = np.array(images)
labels = np.array(labels)

# Converter os rótulos para one-hot encoding
one_hot_labels = to_categorical(labels, num_classes=4)

# Dividir os dados em conjuntos de treino e teste
X_train, X_test, y_train, y_test = train_test_split(images, one_hot_labels, test_size=0.2, random_state=42)

# Construir o modelo de CNN
model = Sequential([
    Conv2D(32, kernel_size=(3, 3), input_shape=(128, 128, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),
    Dropout(0.05),

    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Conv2D(128, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),
    Dropout(0.2),

    Conv2D(256, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),

    Conv2D(512, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),
    Dropout(0.05),

    Flatten(),
    Dense(128, activation='relu'),
    BatchNormalization(),

    Dense(64, activation='relu'),
    Dropout(0.01),

    Dense(4, activation='softmax')  # 4 classes de saída
])

# Compilar o modelo
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Definir o callback de Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Treinar o modelo
history = model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), callbacks=[early_stopping])

# Salvar o modelo
model.save('/content/drive/MyDrive/modelo_cnn.h5')

# Avaliar o modelo
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc}')

# Plotar a precisão e a perda
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Treino')
plt.plot(history.history['val_accuracy'], label='Validação')
plt.xlabel('Época')
plt.ylabel('Acurácia')
plt.legend(loc='lower right')
plt.title('Acurácia do Treinamento e Validação')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Treino')
plt.plot(history.history['val_loss'], label='Validação')
plt.xlabel('Época')
plt.ylabel('Perda')
plt.legend(loc='upper right')
plt.title('Perda do Treinamento e Validação')

plt.show()

# Prever os rótulos do conjunto de teste
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Calcular e plotar a matriz de confusão
cm = confusion_matrix(y_true, y_pred_classes)
cmd = ConfusionMatrixDisplay(cm, display_labels=classes)
cmd.plot(cmap=plt.cm.Blues)
plt.show()

# Caminho para as novas imagens
# Defina o diretório base no Google Drive onde as imagens estão organizadas por classe
base_dir1 = '/content/drive/MyDrive/Alzheimer-Teste'  # ajuste o caminho conforme necessário
classes1 = ['Demente Leve', 'Moderado Demente', 'Não Demente', 'Muito Leve Demente']

# Carregar e pré-processar todas as imagens
images1 = []
labels1 = []

for label, class_name in enumerate(classes1):
    class_dir = os.path.join(base_dir1, class_name)
    for filename in os.listdir(class_dir):
        if filename.endswith('.jpg') or filename.endswith('.png'):
            image_path = os.path.join(class_dir, filename)
            image = preprocess_image(image_path)
            images1.append(image)
            labels1.append(label)

# Converter listas para arrays numpy
images1 = np.array(images1)
labels1 = np.array(labels1)

labels1

# Fazer previsões
predictions = model.predict(images1)
predicted_classes = np.argmax(predictions, axis=1)

# Mapear os índices das classes para os nomes das classes
class_names = ['Demente Leve', 'Moderado Demente', 'Não Demente', 'Muito Leve Demente']
predicted_class_names = [class_names[i] for i in predicted_classes]

# Calcular e plotar a matriz de confusão
cm = confusion_matrix(labels1, predicted_classes)
cmd = ConfusionMatrixDisplay(cm, display_labels=classes1)
cmd.plot(cmap=plt.cm.Blues)
plt.show()